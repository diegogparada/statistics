[["index.html", "Statistics with R Chapter 1 Introduction 1.1 Research stages: 1.2 Variables 1.3 Analysing data 1.4 Further reading", " Statistics with R Diego Guardia Parada 2023-04-09 Chapter 1 Introduction 1.1 Research stages: Generate a research question through an initial observation (hopefully backed up by some data) Generate a theory to explain your initial observation Generate hypotheses: break your theory down into a set of testable predictions. Collect data to test the theory: decide on what variables you need to measure to test your predictions and how best to measure or manipulate those variables. Analyse the data: fit a statistical model to the data - this model will test your original predictions. Asses this model to see whether or nt it supports your initial predictions. 1.2 Variables When doing research there are some important generic terms for variables that you will encounter: Independent variable: A variable thought to be the cause of some effect. Dependent variable: A variable thought to be affected by changes in an independent variable. 1.2.1 Levels of measurement Broadly speaking, variables can be categorical or continuous and can have different levels of measurement. A categorical variable (entities are divided into distinct categories): Binary variables only two distinct types of things (or entities), for example, male or female. Nominal: When two things that are equivalent in some senses are given the same name (or number), but there are more than two possibilities. The only way that nominal data can be used is to consider frequencies. Ordinal: The same as nominal variable but the categories have a logical order. When categories are ordered, this kind of data tell us not only that things have occurred, but also the order in which they occurred. Therefore, ordinal data tell us more than nominal data (they tell us the order in which things happened) but they still do not tell us about the differences between points on a scale. A continuous variable gives us a score for each entity and can take on any value on the measurement scale that we are using. The different types of continuous variable that you might encounter are: Interval: They are considerably more useful than ordinal data. To say that data are interval, we must be certain that equal intervals on the scale represent equal differences in the property being measured. Ratio: They go a step further than interval data by requiring that in addition to the measurement scale meeting the requirements of an interval variable, the ratios of values along the scale should be meaningful. For this to be true, the scale must have a true and meaningful zero point. 1.3 Analysing data When the data are quantitative, this involves both looking at your data graphically to see what the general trends in the data are, and also fitting statistical models to the data. 1.3.1 Frequency distributions Frequency distributions come in many different shapes and sizes. It is quite important, therefore to have some general descriptions for common types of distributions. In an ideal world our data would be distributed symmetrically around the center of all scores. This is know as a normal distribution and is characterized by the bell-shaped curve. This shape implies that the majority of scores lie around the center of the distribution. Also, as we get further away from the center the bars get smaller, implying that as scores start to deviate from the center their frequency is decreasing. As we move still further away from the center our scores become very infrequent. Many naturally occurring things have this shape of distribution. There are two main ways in which a distribution can deviate from normal: 1) lack of symmetry (called skew) and 2) pointyness (called kurtosis). Skewed distributions are not symmetrical and instead the most frequent scores are clustered at one end of the scale. Distributions also vary in their kurtosis. Kurtosis refers to the degree to which scores cluster at t he ends of the distribution (known as the tails) and how pointy a distribution is. A distritution with positiv kurtosis has many scores in the tails and is pointy. This is know as leptokurtic distribution. In contrast, a distribution with negative kurtosis is relatively thin in the tails. In a normal distribution the values of skew and kurtosis are 0. If a distribution has values of skew or kurtosis above or below 0 then this indicates a deviation from nrmal. 1.3.2 The centre of a distribution We can calculate where the centre of a frequency distribution lies (known as the central tendency). There are three measures commonly used: 1.3.2.1 The mode The mode is simply the score that occurs most frequently in the data set. It is also possible to find data sets with more than tw modes (multimodal). 1.3.2.2 The median Another way to quantify the centre of a distribution is t look for the middle score when scores are ranked in order of magnitude. \\[Median=\\frac{(n+1)}{2}\\] This works very nicely when we have an odd number of scores but when we have even number of scores there won’t be a middle value. 1.3.2.3 The mean The mean is the measure of central tendency that you are most likely to have heard of. To calculate the mean we simply add up all of the scores and then divide by the total number f scores we have. We can write this in equation form as: \\[\\overline{X}=\\frac{\\sum_{i=1}^n x_{i}}{n}\\] Meean is affected by extreme scores, it is also affected by skewed distribution and can be used only with interval or ratio data. On the other hand, mean uses every score (the mode and median ignore most of the scores in a data set) and also, mean tends to be stable in different samples. 1.3.3 The dispersion in a distribution It can also be interesting to try to quantify the spread, or dispersion, of scores in the data. The easiest way to look at the dispersion is to take the largest score and subtract from it the smallest score. This is known as the range of scores. One problem with the range is that because it uses only the highest and lowest score it is affected dramatically by extreme scores. One way around this problem is to calculate the range when we exclude values at the extremes of the distribution. One convetion is to cut off the top and bottom 25% of scores and calculate the range of the middle 50% of scores - know as the interquantile range. Quartiles are the three values that split the sorted datta into four equal parts. First we calculate the median, which is also called the second quartile, which splits our data into two equal parts. The lower quartile is the median of the lower half of the data and the upper quartile is the median of the upper half of the data. One rule of thumb is that the median is not included in the two halves when they are splid (inconvinient if you have an dd number of values) but you can include it (althought which half you put it in anther question) 1.3.4 Using a frequency distribution to go beyond the data For any distribution of scores we could, in theory, calculate the probability of obtaining a score of a certain size - it would be incredible tedious and complex to do it, but we could. To spare our sanity, staticians have identified several common distributions. For each one they have worked out mathematical formulae that specify idealized versiones of these distributions (they are specified in terms of a curved line). These idealized distributions are known as probability distributions and from these distributions it is possible to calculate the probability of getting particular scores based on the frequencies with which a particular score occurs in a distribution with these common shapes. One of these common distributions is the normal distribution. Staticians have calculated the probability of certain scores ocurring in a normal distribution with a meanof 0 and a standard deviation of 1. Therefore, if we have any data that are shaped like shaped like a normal distribution, then if the mean and standard deviation are 0 and 1 respectively we can use the tables of probabilities for the normal distribution to see how likely it is that a particular score will occur in the data. The obvious problem is that not all of the data we collect will have a mean of 0 and standard deviation of 1. Luckily any data set can be converted into a data set that has a mean of 0 and a standard deviation of 1. First, to centre the data around zero, we take each score (X) and subtract from it the mean of all scores \\(\\overline{X}\\). Then, we divide the resulting score by the standard deviation to ensure the data have a standard deviation of 1. The resulting scores are known as z-scores and, in equation form, the conversion that has been described is: \\[z=\\frac{X-\\overline{X}}{s}\\] The normal distribution and z-scores allow us to go a first step beyond our data in that from a set of scores we can calculate the probability that a particular score will occur. You will see just how useful this is in due course, but it is worth mentioning at this stage that certain z-scores are particularly important: z is 1.96: Cuts off the top 2.5% of the distribution, and its counterpart at the opposite end (-1.96) cuts off the bottom 2.5% of the distribution. As such, taken together, this value cuts off 5% of scores, or, put another way, 95% of z-scores lie between -1.96 and 1.96. z is 2.58: Cuts off the top 0.5% of the distribution, and its counterpart at the opposite end (-2.58) cuts off the bottom 0.5% of the distribution. As such, taken together, this value cuts off 1% of scores, or, put another way, 95% of z-scores lie between -2.58 and 2.58. z is 3.29: Cuts off the top 0.05% of the distribution, and its counterpart at the opposite end (-3.29) cuts off the bottom 0.05% of the distribution. As such, taken together, this value cuts off 0.1% of scores, or, put another way, 95% of z-scores lie between -3.29 and 3.29. 1.3.5 Hypothesis The hypothesis or prediction that comes from your theory is usually saying that an effect will be present. This hypothesis is called the alternative hypothesis and is denoted by \\(H_{1}\\). There is another type of hypothesis and is called the null hypothesis and is denoted by \\(H_{0}\\). This hypothesis is the opposite of the alternative hypothesis and so would usually state that an effect is absent. The reason that we need the null hypothesis is because we cannot prove the experimental hypothesis using statistics, but we can reject the null hypothesis. If our data give us confidence to reject the null hypothesis then this provides support for our experimental hypothesis. However, be aware that even if we can reject the null hypothesis, this doesn’t prove the experimental hypothesis - it merely supports it. So, rather than talking about accepting or rejecting a hypothesis we should be talking about the chances of obtaining the data we have collected assuming that the null hypothesis is true. 1.4 Further reading Field, A. P., &amp; Hole, G. J. (2003). How to design and report experiments. London: Sage. Miles J. N. V., &amp; Banyard, P. (2007) Undersanding and using statistics in psychology: a practical introduction. London: Sage. Wright, D. B., &amp; London, K. (2009). First steps in statistics (2nd ed.). London: Sage. "],["basic-statistical-concepts.html", "Chapter 2 Basic statistical concepts 2.1 Simple statistical models 2.2 Going beyond the data 2.3 Confidence intervals 2.4 Test statistics 2.5 Type I and Type II errors 2.6 Effect sizes 2.7 Futher reading", " Chapter 2 Basic statistical concepts As researchers, we are interested in finding results that apply to an entire population of people or things. Scientists rarely, if ever, have access to every member of a population. Therefore, we collect data from a small subset of the population (known as a sample) and use these data to infer things about the populaation as a whole. The bigger the sample, the more likely it is to reflect the whole population. If we take several random samples from the population, each of these samples will give us slightly different results. However, on average, large samples should be fairly similar. 2.1 Simple statistical models 2.1.1 The mean: a very simple statistical model The mean is a statistical model of the data because it is a hypothetical value that doesn’t have to be a value that is actually observed in the data. As such, the mean is a model created to summarize our data. 2.1.2 Asessing the fit of the mean: sums f squares, variance and standard deviations With any statistical model we have to assess the fit. With most statistical models we can determine whether the model is accurate by looking at how different our real data are from the model that we have created. The easiest way to do this is to look at the difference between the data we observed and the model fitted. The easiest way to do this is to look at the differencee between the data we observed and the model fitted. How can we use deviances to estimate the accuracy of a model? One possibility is to add up the deviances (this would give us an estimate of the total error). If we were to do this, we would find that: \\[\\sum(x_{i}-\\overline{x})=0\\] So, in effect the result tells us that there is no total error between our model and the observed data, so t he mean is a perfect representation of the data. Now this clearly isn’t true: there were error but some of them were positive, some were negative and they have simply cancelled each other out. It is clear that we need to avoid the problem of which direction the error is in and one mathematical way to do this is to square each error, that is multiply each error by itself. So, rather than calculating the sum of errores, we calculate the sum of squared errors: \\[\\sum(x_{i}-\\overline{x})(x_{i}-\\overline{x})=0\\] The sum of squared errors (SS) is a good measure of the accuracy of our model. However, it is fairly bvious that the sum of squared errrs is dependent upon the amount of data that has been cllected - the mor data points, the higher the SS. T overcome this problem, we calculate the average error by dividing the SS by the number of observations (N). If we are interested only in the average error for the sample, then we can divide by N alone. However, we are generally interested in using the error in the sample to estimate the error in the population and so we divide the SS by the number of observations minus 1. This measure is know as the variance and is a measure that we will come across a great deal: \\[variance(s^2)=\\frac{SS}{N-1}=\\frac{\\sum(x_{i}-\\overline{x})^2}{N-1}\\] #### Degrees of freedom Degrees of freedom refer to the number of values in a calculation that are free to vary without affecting the outcome of the calculation. For example, let’s say you have a sample of 10 numbers and you want to calculate the sample variance. To do this, you need to subtract the mean of the sample from each individual number, square those differences, add them up, and divide by the number of degrees of freedom. The number of degrees of freedom in this calculation is \\(N-1\\), where N is the sample size. In other words, you have 9 degrees of freedom because once you have calculated the mean of the sample, you only have 9 numbers that can vary freely, since the sum of all the numbers in the sample must be fixed. Degrees of freedom are important because they affect the precision of statistical estimates. When there are fewer degrees of freedom, there is less information available to make an accurate estimate, so statistical tests and confidence intervals become less reliable. The variance is, therefore, the average error between the mean and the observations made. There is one problem with the variance as a measure: it gives us a measure in units squared (because we squared each error in the calculation). That is why we take the square root of the variance, this measure is know as the standard deviation: \\[s=\\sqrt{\\frac{\\sum(x_{i}-\\overline{x})^2}{N-1}}\\] The sum of squares, variance and standard deviation are all, therefore, measures of the fit (how well the mean represents the data). Small standard deviations indicate that data points are close to the mean. A large standard deviation indicates that the data points are distant from the mean (the mean is not an accurate representation of the data). A standard deviation of 0 would mean that all of the scores were the same. It is also worth noting that the variance and standard deviation also tell us about the shape of the distribution of scores, as the standard deviation gets larger, the distribution gets fatter, a small standard deviation relative to the mean results in a more pointy distribution in which scores close to the mean are very frequent. 2.1.2.1 Expressing the mean as a model Everything in statistics essentially boils down to one quation: \\[outcome_{i}=model+error_{i}\\] This just means that the data we observe can be predicted from the model we choose to fit to the data plus some amount of error. You will discover taht most things ultimately boil down to this one simple idea! Likewise, the variance and standard deviation illustrate another fundamental concept: how the goodness of fit of a model can be measured. If we are looking at how well a model fits the data the we generally look at deviation from the model, we look at the summ of squared error, and in general terms we can write this: \\[deviation=\\sum (observed-model)^2\\] We assess models by comparing the data we observe to the model we have fitted to the data, and then square these differences. 2.2 Going beyond the data 2.2.1 The standard error Many students get confused about the difference between the santard deviation and the standard error. We have explained before that scientist use samples as a way of estimating the behaviour in a population. When someone takes a sample from a population, they are taking one of many possible samples. If we were to take several samples from t he same population, then each sample has its own mean, and some of these sample means will be different. Samples will vary because they contain different members of the population, this is known as sampling variation. We can actually plot the sample means as a frequency distribution, or histogram. The end result is a nice symmetrical distribution known as a sampling distribution. A sampling distribution is simply the frequency distribution of sample means (it doesn’t have to be means, it can be any statistic that you are trying to estimate). The sampling distrbution tells us about the behaviour of samples from t he population and you will notice that it is centred at the same value as the mean of the population. This means that, if we took the average of all sample means we’d get the value of the population mean. Now,if the average of the sample means is the same value as the population mean, then if we knew the accuracy of that average we’d know something about how likely it is that a given sample is representative of the population. So how do we determine the accuracy of the population mean? We used the standard deviation as a measure of how represtantive the mean was of the observed rdata. Small standard deviations represented a scenario in which most data points were close to the mean, a large standard deviation represented a situation in which data points were widely spread from the team. If you were to calculate the standard deviation between sample means then this too would give you a mesaure of how much variablity there was between the means of different samples. The standard deviation of sample means is know as the standard error of the mean (SE). Therefore, the standard error could be calculated by taking the difference between each sample mean and the overall mean, squaring these difference, adding them up and then dividngby the number of samples. Finally, the square root of this value would need to be taken t get the standard deviation of sample means, the standard error. Of course, in reality we cannot collect hundreds of samples and so we rely on approximations of the standard error. Clever staticians have demonstrated that as samples get large (usually defined as greater than 30) the sampling distribution has a normal distribution with a mean equal to the population mean, and a standard deviation of: \\[SE=\\sigma_{\\overline{X}}=\\frac{s}{\\sqrt{N}}\\] The standard error of the mean (SE) is a measure of the variability of the sample mean, which is an estimate of the population mean. It is calculated as the standard deviation of the sample divided by the square root of the sample size. The SE is important because it tells us how much the sample mean is likely to differ from the true population mean. A smaller SEM indicates that the sample mean is a more precise estimate of the population mean. The SE is commonly used in inferential statistics to calculate confidence intervals and to determine the statistical significance of differences between sample means. This is known as the central limit theorem and it is useful in this context because it means that if our sample is large we can use the above euqation to approximate the santard error (because, remember, it is the standard deviation of the sampling distribution). When the sample is relatively small (fewer than 30) the sampling distribution has a different shape, known as a t-distribution, which we will come later. The standard error is the standard deviation of sample means. As such, it is a measure of how representative a sample is likely to be of the population. A large standard error means that there is a lot of variablity between the means of different samples and so the sample we have might not be representative f the population. A small standard error indicates that most sample means are similar to the population mean and so our sample is likely to be an accurate reflection of the population. 2.3 Confidence intervals 2.3.1 Calculating confidence intervals A different approach to assesing the accuracy of the sample mean as an estimate of the mean in the population is to calculate boundaries within which we believe the true value of the mean will fall. Such boundaries are called confidence intervals. The basic idea behind confidence intervals is to construct a range of value within which we think the population value falls. Before we construct confidence intervals, we first have to note that the sample means are different from the true mean (because of sampling variation, as described before). Second, although most of the intervals do contain the true mean, a few do not. We calculate them so that they contain certain properties: they tell us the likelihood that they contain t he true value of the thing we are trying to estimate (in this case, the mean). Tipically we look at 95% confidence intervals and sometimes 99% confidence intervals, but they all have a similar interpretation: they are limits constructed such that for a certain percentage of th time (95% or 99%) the true value of the population mean will fall within these limits. So when you see a 95% confidence interval for a mean, think of it like this: if we0d collected 100 samples, calculated the mean and then calculated a confidence intervals we constructed would contain the true value of the mean in the population. To calculate the confidence interval, we need to know the limits within which 95% of means will fall. Remember that 1.96 was an important value of z (a score from a normal distribution with a mean of 0 and standard deviation of 1) because 95% of z-scores fall between -1.96 and 1.96. Luckily we know from the central limit theorem that in large samples (above about 30) the sampling distribution will be normally distributed. Our mean and standard deviation are unlikely to be 0 and 1; except not really because, as you might remember, we can convert scores so that they do have a mean of 0 and standard deviation of 1 (z-scores): \\[z=\\frac{X-\\overline{X}}{s}\\] If we know that our limits are -1.96 and 1.96 in z-scores, then to find out the corresponding scores in our raw data we can replace z in the equation: \\[1.96=\\frac{X-\\overline{X}}{s}\\] \\[-1.96=\\frac{X-\\overline{X}}{s}\\] We rearrange these equation to discover the value of X: \\[(1.96*s)+\\overline{X}=X\\] \\[(-1.96*s)+\\overline{X}=X\\] Therefore, the confidence interval can easily be calculated once the standard deviation and mean are known. However, we use the standard error and not the standard deviation because we are interested in the variability of sample means, not the variability within the sample: \\[lower~boundary=\\overline{X}-(1.96*SE)\\] \\[upper~boundary=\\overline{X}+(1.96*SE)\\] As such, the mean is always in the centre of the confidence interval. If the mean represents the true mean well, then the confidence interval of that mean should be small. If the interval is small, the sample mean must be very close to the true mean. If the confidence interval is very wide then the sample mean could be very different from the true mean, indicating that it is a bad representation of the population. 2.3.2 Calculating other confidence intervals In general, we could say that confidence intervals are calculated as: \\[lower~boundary=\\overline{X}-(z_{\\frac{1-p}{2}}*SE)\\] \\[upper~boundary=\\overline{X}+(z_{\\frac{1-p}{2}}*SE)\\] in which p is the probability value for the confidence interval. So, if you want a 95% confidence interval, then you want t he value of \\(z_{\\frac{1-0.95}{2}}=0.025\\), we have to look for this value in the table of the standard normal distribution. 2.3.3 Calculating confidence intervals in small samples For small samples, the sampling distribution is not normal, it has a t-distribution. The t-distribution is a family of probabilityt distributions that change shape as the sample size gets bigger (when the sample is very big, it has the shape of a normal distribution). To construct a confidence interval in a small sample we use the same principle as before but instead of using the value for z we use the value for t: \\[lower~boundary=\\overline{X}-(t_{n-1}*SE)\\] \\[upper~boundary=\\overline{X}+(t_{n-1}*SE)\\] The \\(n-1\\) in the equations is the degrees of freedom and tells us which of the t-distributions to use. For a 95% confidence interval we find the value of t for a tw-tailed test with probability of .05 (5%), for the appropiate degrees of freedom. A confidence interval for the mean is a range of scores constructed such that the population mean will fall within this range in 95% of samples. The confidence interval is not an interval within which we are 95% confident that the population mean will fall. 2.4 Test statistics We can test whether our statistical models (and therefore our hypotheses) are significant fits of the data we collected. To do this, we need to see the concepts of systematic and unsystematic variation. Systematic variation is variation that can be explained by the model that we’ve fitted to the data (and therefore, due to the hthe hypothesis that we’re testing). Unsystematic variation is variation that cannot be explained by the model that we’ve fitted. In other words, it is error, or variation not attributable to the effect we’re investigating. The simplest way to test whether the model fits the data, or whether our hypothesis is a good explanation of the data we have observed is to compare the systematic variation against the unsystematic variation. In doing so we compare how good the model/hypothesis is at explaining the data against how bad it is (the error): \\[test~statistic=\\frac{variance~explained~by~the~model}{variance~not~explained~by~the~model}=\\frac{effect}{error}\\] This ratio of systematic to unsystematic variance or effect to error is a test statistic, and you’ll discover later in the book there are lots of them: t, F and \\(\\chi2\\) to name only three. The exact form of the equation changes depending on which test statistic you’re calculating, but the important thing to remember is that they all represent the same thing: the amount of variance explained by the model we’ve fitted to the data compared to the variance that can’t be explained by the model. The reason why this ratio is so useful is intuitive really: if our model is good then we’d expect it to be able to explain more variance that it can’t explain. In this case, the test statistic will be greater than 1 (but no necessarily significant). A test statistic is a statistic that has known properties; specifically, we know how frequently different values of this statistic occur. This allows us to establish how likely it would be that we would get a test statistic of a certain size if there were no effect (i.e., the null hypothesis were true). We know their distributions and this allows us, once we’ve calculated the test statistic, to discover the prbability of having found a value as big as we have. The more variation our model explains (compared to the variance it can’t explain), the bigger the test statistic will be, and the more unlikely it is to occur by chance. So, as test statistics get bigger, the probability of them ocurring becomes smaller. When this probability falls below .05 (Fisher’s criterion), we accept this as giving us enough confidence to assume that the test statistic is as large as it is because our model explains a sufficient amount of variation to reflect what’s genuinely happening in the real world (the population). A significance level of 0.05 (also known as alpha level or level of significance) is commonly used in hypothesis testing. This means that the probability of obtaining a test statistic as extreme or more extreme than the one observed, assuming the null hypothesis is true, is less than 5%. If the test statistic falls below this level, it is considered statistically significant and the null hypothesis is rejected. The significance level of 0.05 is chosen because it provides a balance between making correct rejections of the null hypothesis (i.e., avoiding type II errors) and avoiding false rejections of the null hypothesis (i.e., avoiding type I errors). Type I error occurs when the null hypothesis is rejected when it is actually true. Type II error occurs when the null hypothesis is not rejected when it is actually false. The level of 0.05 ensures that the probability of making a type I error is controlled at 5%. In summary, a significance level of 0.05 is important because it provides a standard threshold for determining whether a result is statistically significant, and helps to balance the trade-off between making correct rejections of the null hypothesis and avoiding false rejections. Given that the statistical model thatt we fit to the data reflects the hypothesis that we set out to test, then a significant test statistic tells us that the model would be unlikely to fit this well if there was no effect in the population (i.e., the null hypothesis was true). Therefore, we can reject our null hpyothesis and gain confidence that the alternative hypothesis is true (but, remember, we don’t accept it). ## One and two-tailed test Hypotheses can be directional or non-directional. A statistical model that test a directional hypothesis is called a one-tailed test (i.e. the more someone reads this book, the more they want to kill its author), whereas one testing a non-directional hypothesis (i.e. reading more of this book could increase or decrease the reader’s desire to kill its author) is known as a two-tailed test. 2.5 Type I and Type II errors A Type I error occurs when we believe that there is a genuine effect in our population, when in fact there isn’t. If we use Fisher’s criterion then the probability of this error is .05 (or 5%) when there is no effect in the population - this value is known as the \\(\\alpha -level\\). Assuming there is no effect in ur population, if we replicated our data collection 100 times we could expect that on five occasions we would obtain a test statistic large enough to make us think that there was a genuine effect in the population even thought there isn’t. The opposite is a Type II error which occurs when we believe that there is no effect int he population when, in reality, there is. The maximum acceptable probability of a Type II error would be .2 (or 20%) - this is called the \\(\\beta -level\\). 2.6 Effect sizes Just because a test statistic is significant doesn’t mean that the effect it measures is meaningful or important. The solution is to to measure the size of the effect that we’re testing in a standardized way. The fact that the measure is standardized just means that we can compare effect sizes across different studies that have measured different variables, or have used different scales of measurement. Many measures of effect size have been proposed, the most common of which are Cohen’s d, Pearon’s correlation coefficient r and the odds ratio. Effect sizes are useful because they provide an objective measure of the importance of an effect. Cohen has also made some widely used suggestions about what constitutes a large or small effect: \\(r=.10\\) (small effect): The effect explains 1% of the total variance. \\(r=.30\\) (medium effect): The effect explains 9% of the total variance. \\(r=.50\\) (large effect): The effect explains 25% of the total variance. 2.7 Futher reading Cohen, J. (1994). The earth is round (p&lt;.95). American Psychologist, 49(12), 997-1003. Wright, D. B., &amp; London, K. (2009). First steps in statistics (2nd ed.) London: Sage. "],["the-r-environment.html", "Chapter 3 The R environment 3.1 Before you start 3.2 Using R 3.3 Getting data into R", " Chapter 3 The R environment 3.1 Before you start R is a free software environment for statistical computing and graphics. It is what’s known as open source, which means that people who developed R allow everyone to access their code. This open source philosophy allows anyone, anywhere to contribute to the software. Consequently, the capabilities of R dynamically expand as people from all over the world add to it. ### The R-chitecture The beauty of R is that it can be expanded by downloading packages that add specific functionality to the program. These packages as well as the software itself, are stored in a central location known as the CRAN (Comprehensive R Archive Network) . Once a package is stored in the CRAN, anyone with an internet connection can download it from CRAN. ### Pros and cons of R The main advantages of using R are that it is free, and it is a versatile and dynamic environment. In addition, it is a rapidly expanding tool and can respond quickly to new developments in data analysis. The downside to R is mainly ease of use. The ethos of R is to work with a command line rather than a graphical user interface (GUI). In layman’s terms this means typing instructions rather than pointing, clicking and draggin things with a mouse. 3.2 Using R 3.2.1 Commands, objects and functions Commands in R are generally made up of two parts: objects and functions. These are separated by ‘&lt;-’, which you can think of as meaning ‘is created from’. An object is anything created in R. It could be a variable, a collection of variables, a statistical model, etc. Functions are the things that you do in R to create your objects. 3.2.2 Running multiple commands at once It can be useful to run several commands in a single line. Separating them with a semicolon does this. For example, the last two commands: metallica&lt;-c(&quot;Lars&quot;, &quot;James&quot;, &quot;Jason&quot;, &quot;Kirk&quot;) metallica&lt;-metallica[metallica != &quot;Jason&quot;] metallica&lt;-c(metallica, &quot;Rob&quot;) can be run in a single line by using a semicolon to separate them: metallica&lt;-metallica[metallica != &quot;Jason&quot;]; metallica&lt;-c(metallica, &quot;Rob&quot;) 3.2.3 R is case sensitive R is case sensitive, which means that if the same things are written in upper or lower case, R thinks that they are completely different things. For example, we created a variable called metalica; if we asked to see the contents of Metallica (note the capital M), R would tell us that this object didn’t exist. 3.2.4 Setting a working directory To set a working directory, we use setwd() command to specify a newly created folder as the working directory, then we see the new working directory by typing getwd(): setwd(&quot;C:/Users/dguar/Desktop/statistics&quot;) getwd() ## [1] &quot;C:/Users/dguar/Desktop/statistics&quot; By executing this command, we can now access files in that folder directly without having to reference the full file path. For example, if we wanted to load our data.dat file again, we can now execute this command: myData=read.delim(&quot;data.dat&quot;) 3.2.5 Installing packages R comes with some base functions ready for you to use. However, to get the most out of it, we need to install packages that enable us to do particular things. You can install packages in two ways: through the menus or using a command. If you know the package that you want to install then the simplest way is to execute this command: install.packages(“package.name”) You need to install the package only once but you need to reference it each time you star a new session of R. To reference a package, we simply execute this general command: library(“package.name”) 3.2.6 Disambiguating functions Ocassionally you might stumble across two functions in two different packages that have the same name. For example, there is a recode() function in both the Hmisc andcar packages. If you hae both packages loaded and you try to use recode(), R won’t know which one to use or will have a guess. This istuation is easiy to recitfy: you can specify the package when you use the function as follows: package::function() 3.2.7 Getting help If you are using a particular function and you want to know more about it then you can get help by executing the help() command like this: help(function) or by executing ?function. If you try to use help but the help files are not found, check that you have loaded the relevant package with the library() command. 3.3 Getting data into R "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
