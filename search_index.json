[["index.html", "Statistics with R Chapter 1 Introduction 1.1 Variables 1.2 Analysing data 1.3 Further reading", " Statistics with R Diego Guardia Parada 2023-04-02 Chapter 1 Introduction 1.1 Variables When doing research there are some important generic terms for variables that you will encounter: Independent variable: A variable thought to be the cause of some effect. Dependent variable: A variable thought to be affected by changes in an independent variable. 1.1.1 Levels of measurement Broadly speaking, variables can be categorical or continuous and can have different levels of measurement. A categorical variable (entities are divided into distinct categories): Binary variables only two distinct types of things (or entities), for example, male or female. Nominal: When two things that are equivalent in some senses are given the same name (or number), but there are more than two possibilities. The only way that nominal data can be used is to consider frequencies. Ordinal: The same as nominal variable but the categories have a logical order. When categories are ordered, this kind of data tell us not only that things have occurred, but also the order in which they occurred. Therefore, ordinal data tell us more than nominal data (they tell us the order in which things happened) but they still do not tell us about the differences between points on a scale. A continuous variable gives us a score for each entity and can take on any value on the measurement scale that we are using. The different types of continuous variable that you might encounter are: Interval: They are considerably more useful than ordinal data. To say that data are interval, we must be certain that equal intervals on the scale represent equal differences in the property being measured. Ratio: They go a step further than interval data by requiring that in addition to the measurement scale meeting the requirements of an interval variable, the ratios of values along the scale should be meaningful. For this to be true, the scale must have a true and meaningful zero point. 1.2 Analysing data When the data are quantitative, this involves both looking at your data graphically to see what the general trends in the data are, and also fitting statistical models to the data. 1.2.1 Frequency distributions Frequency distributions come in many different shapes and sizes. It is quite important, therefore to have some general descriptions for common types of distributions. In an ideal world our data would be distributed symmetrically around the center of all scores. This is know as a normal distribution and is characterized by the bell-shaped curve. This shape implies that the majority of scores lie around the center of the distribution. Also, as we get further away from the center the bars get smaller, implying that as scores start to deviate from the center their frequency is decreasing. As we move still further away from the center our scores become very infrequent. Many naturally occurring things have this shape of distribution. There are two main ways in which a distribution can deviate from normal: 1) lack of symmetry (called skew) and 2) pointyness (called kurtosis). Skewed distributions are not symmetrical and instead the most frequent scores are clustered at one end of the scale. Distributions also vary in their kurtosis. Kurtosis refers to the degree to which scores cluster at t he ends of the distribution (known as the tails) and how pointy a distribution is. A distritution with positiv kurtosis has many scores in the tails and is pointy. This is know as leptokurtic distribution. In contrast, a distribution with negative kurtosis is relatively thin in the tails. In a normal distribution the values of skew and kurtosis are 0. If a distribution has values of skew or kurtosis above or below 0 then this indicates a deviation from nrmal. 1.2.2 The centre of a distribution We can calculate where the centre of a frequency distribution lies (known as the central tendency). There are three measures commonly used: 1.2.2.1 The mode The mode is simply the score that occurs most frequently in the data set. It is also possible to find data sets with more than tw modes (multimodal). 1.2.2.2 The median Another way to quantify the centre of a distribution is t look for the middle score when scores are ranked in order of magnitude. \\[Median=\\frac{(n+1)}{2}\\] This works very nicely when we have an odd number of scores but when we have even number of scores there won’t be a middle value. 1.2.2.3 The mean The mean is the measure of central tendency that you are most likely to have heard of. To calculate the mean we simply add up all of the scores and then divide by the total number f scores we have. We can write this in equation form as: \\[\\overline{X}=\\frac{\\sum_{i=1}^n x_{i}}{n}\\] Meean is affected by extreme scores, it is also affected by skewed distribution and can be used only with interval or ratio data. On the other hand, mean uses every score (the mode and median ignore most of the scores in a data set) and also, mean tends to be stable in different samples. 1.2.3 The dispersion in a distribution It can also be interesting to try to quantify the spread, or dispersion, of scores in the data. The easiest way to look at the dispersion is to take the largest score and subtract from it the smallest score. This is known as the range of scores. One problem with the range is that because it uses only the highest and lowest score it is affected dramatically by extreme scores. One way around this problem is to calculate the range when we exclude values at the extremes of the distribution. One convetion is to cut off the top and bottom 25% of scores and calculate the range of the middle 50% of scores - know as the interquantile range. Quartiles are the three values that split the sorted datta into four equal parts. First we calculate the median, which is also called the second quartile, which splits our data into two equal parts. The lower quartile is the median of the lower half of the data and the upper quartile is the median of the upper half of the data. One rule of thumb is that the median is not included in the two halves when they are splid (inconvinient if you have an dd number of values) but you can include it (althought which half you put it in anther question) 1.2.4 Using a frequency distribution to go beyond the data For any distribution of scores we could, in theory, calculate the probability of obtaining a score of a certain size - it would be incredible tedious and complex to do it, but we could. To spare our sanity, staticians have identified several common distributions. For each one they have worked out mathematical formulae that specify idealized versiones of these distributions (they are specified in terms of a curved line). These idealized distributions are known as probability distributions and from these distributions it is possible to calculate the probability of getting particular scores based on the frequencies with which a particular score occurs in a distribution with these common shapes. One of these common distributions is the normal distribution. Staticians have calculated the probability of certain scores ocurring in a normal distribution with a meanof 0 and a standard deviation of 1. Therefore, if we have any data that are shaped like shaped like a normal distribution, then if the mean and standard deviation are 0 and 1 respectively we can use the tables of probabilities for the normal distribution to see how likely it is that a particular score will occur in the data. The obvious problem is that not all of the data we collect will have a mean of 0 and standard deviation of 1. Luckily any data set can be converted into a data set that has a mean of 0 and a standard deviation of 1. First, to centre the data around zero, we take each score (X) and subtract from it the mean of all scores \\(\\overline{X}\\). Then, we divide the resulting score by the standard deviation to ensure the data have a standard deviation of 1. The resulting scores are known as z-scores and, in equation form, the conversion that has been described is: \\[z=\\frac{X-\\overline{X}}{s}\\] The normal distribution and z-scores allow us to go a first step beyond our data in that from a set of scores we can calculate the probability that a particular score will occur. You will see just how useful this is in due course, but it is worth mentioning at this stage that certain z-scores are particularly important: z is 1.96: Cuts off the top 2.5% of the distribution, and its counterpart at the opposite end (-1.96) cuts off the bottom 2.5% of the distribution. As such, taken together, this value cuts off 5% of scores, or, put another way, 95% of z-scores lie between -1.96 and 1.96. z is 2.58: Cuts off the top 0.5% of the distribution, and its counterpart at the opposite end (-2.58) cuts off the bottom 0.5% of the distribution. As such, taken together, this value cuts off 1% of scores, or, put another way, 95% of z-scores lie between -2.58 and 2.58. z is 3.29: Cuts off the top 0.05% of the distribution, and its counterpart at the opposite end (-3.29) cuts off the bottom 0.05% of the distribution. As such, taken together, this value cuts off 0.1% of scores, or, put another way, 95% of z-scores lie between -3.29 and 3.29. 1.2.5 Hypothesis The hypothesis or prediction that comes from your theory is usually saying that an effect will be present. This hypothesis is called the alternative hypothesis and is denoted by \\(H_{1}\\). There is another type of hypothesis and is called the null hypothesis and is denoted by \\(H_{0}\\). This hypothesis is the opposite of the alternative hypothesis and so would usually state that an effect is absent. The reason that we need the null hypothesis is because we cannot prove the experimental hypothesis using statistics, but we can reject the null hypothesis. If our data give us confidence to reject the null hypothesis then this provides support for our experimental hypothesis. However, be aware that even if we can reject the null hypothesis, this doesn’t prove the experimental hypothesis - it merely supports it. So, rather than talking about accepting or rejecting a hypothesis we should be talking about the chances of obtaining the data we have collected assuming that the null hypothesis is true. 1.3 Further reading Field, A. P., &amp; Hole, G. J. (2003). How to design and report experiments. London: Sage. Miles J. N. V., &amp; Banyard, P. (2007) Undersanding and using statistics in psychology: a practical introduction. London: Sage. Wright, D. B., &amp; London, K. (2009). First steps in statistics (2nd ed.). London: Sage. "],["basic-statistical-concepts.html", "Chapter 2 Basic statistical concepts", " Chapter 2 Basic statistical concepts "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
